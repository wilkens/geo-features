{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic features from Hathi volumes\n",
    "\n",
    "Create geographical similarity features in a list of volumes identified by HTIDs. Methods of comparison and number of output features customizable.\n",
    "\n",
    "## Imports and setup\n",
    "\n",
    "Note that if we're using live data from the Textual Geographies database, we need to first establish a port-forwarding SSH connection:\n",
    "\n",
    "```\n",
    "ssh mwilkens@geolit.crc.nd.edu -L 5433:localhost:5433\n",
    "```\n",
    "\n",
    "Can skip this if pulling data from local files (set variable `use_db=False` in the first code block below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from   sklearn.decomposition import PCA\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "from   IPython.display import display\n",
    "import string \n",
    "\n",
    "# Variables and options\n",
    "input_dir = 'inputs'\n",
    "results_dir = 'results'\n",
    "fig_dir = 'figures'\n",
    "use_db = False # Pull live data from Postgres? (else use local CSV)\n",
    "metadata_filename = 'rh.csv'\n",
    "postgres_credentials = '/Users/mwilkens/Google Drive/Private/postgresql-96-credentials.txt'\n",
    "postgres_server      = 'localhost'\n",
    "postgres_db_name     = 'research'\n",
    "postgres_port        = '5433'\n",
    "\n",
    "# Setup\n",
    "%matplotlib inline\n",
    "metadata_file = os.path.join(input_dir, metadata_filename)\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "plt.rc('figure', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>HATHI_ID</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>RACE</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>YEAR_CORRECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Christmas Book</td>\n",
       "      <td>Anglund, Joan Walsh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1950</td>\n",
       "      <td>F</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20,000 Baseball Cards Under The Sea</td>\n",
       "      <td>Buller, Jon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian?</td>\n",
       "      <td>American</td>\n",
       "      <td>YA</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Christmas Memory</td>\n",
       "      <td>Capote, Truman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7Th Heaven</td>\n",
       "      <td>Christie, Amanda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1950</td>\n",
       "      <td>F</td>\n",
       "      <td>white</td>\n",
       "      <td>American</td>\n",
       "      <td>ROM</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20Th-Century Dreams</td>\n",
       "      <td>Cohn, Nik</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>British</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 TITLE               AUTHOR HATHI_ID  \\\n",
       "0                     A Christmas Book  Anglund, Joan Walsh      NaN   \n",
       "1  20,000 Baseball Cards Under The Sea          Buller, Jon      NaN   \n",
       "2                   A Christmas Memory       Capote, Truman      NaN   \n",
       "3                           7Th Heaven     Christie, Amanda      NaN   \n",
       "4                  20Th-Century Dreams            Cohn, Nik      NaN   \n",
       "\n",
       "      PUBLISHER  YEAR GENDER        RACE NATIONALITY GENRE YEAR_CORRECT  \n",
       "0  Random House  1950      F   Caucasian    American   LIT         1983  \n",
       "1  Random House  1950      M  Caucasian?    American    YA         1991  \n",
       "2  Random House  1950      M   Caucasian    American   LIT         1956  \n",
       "3  Random House  1950      F       white    American   ROM         2000  \n",
       "4  Random House  1950      M   Caucasian     British   LIT         1999  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>HATHI_ID</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>RACE</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>YEAR_CORRECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3525</td>\n",
       "      <td>3506</td>\n",
       "      <td>1371</td>\n",
       "      <td>3525</td>\n",
       "      <td>3525</td>\n",
       "      <td>3425</td>\n",
       "      <td>3309</td>\n",
       "      <td>3341</td>\n",
       "      <td>3503</td>\n",
       "      <td>3524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3525</td>\n",
       "      <td>2023</td>\n",
       "      <td>1371</td>\n",
       "      <td>348</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A man to conjure with, | a novel.</td>\n",
       "      <td>Osborne, Mary Pope</td>\n",
       "      <td>uc1.32106002180732</td>\n",
       "      <td>Random House</td>\n",
       "      <td>2000</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>LIT</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2154</td>\n",
       "      <td>180</td>\n",
       "      <td>1359</td>\n",
       "      <td>2068</td>\n",
       "      <td>1771</td>\n",
       "      <td>995</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TITLE              AUTHOR  \\\n",
       "count                                3525                3506   \n",
       "unique                               3525                2023   \n",
       "top     A man to conjure with, | a novel.  Osborne, Mary Pope   \n",
       "freq                                    1                  50   \n",
       "\n",
       "                  HATHI_ID     PUBLISHER  YEAR GENDER       RACE NATIONALITY  \\\n",
       "count                 1371          3525  3525   3425       3309        3341   \n",
       "unique                1371           348   111     16         27         100   \n",
       "top     uc1.32106002180732  Random House  2000      M  Caucasian    American   \n",
       "freq                     1          2154   180   1359       2068        1771   \n",
       "\n",
       "       GENRE YEAR_CORRECT  \n",
       "count   3503         3524  \n",
       "unique    13          142  \n",
       "top      LIT         2000  \n",
       "freq     995          228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3525 entries, 0 to 3524\n",
      "Data columns (total 10 columns):\n",
      "TITLE           3525 non-null object\n",
      "AUTHOR          3506 non-null object\n",
      "HATHI_ID        1371 non-null object\n",
      "PUBLISHER       3525 non-null object\n",
      "YEAR            3525 non-null object\n",
      "GENDER          3425 non-null object\n",
      "RACE            3309 non-null object\n",
      "NATIONALITY     3341 non-null object\n",
      "GENRE           3503 non-null object\n",
      "YEAR_CORRECT    3524 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 275.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in corpus metadata\n",
    "meta = pd.read_csv(metadata_file, sep=',')\n",
    "\n",
    "# Basic stats on corpus counts and publication dates\n",
    "display(meta.head())\n",
    "display(meta.describe())\n",
    "display(meta.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of cleanup on the metadata that could happen here, but we only need the list of HTIDs, so not doing anything else.\n",
    "\n",
    "Get list of known HTIDs with which to work ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volumes with HTID: 1371 (of 3525 total; 38.9 percent coverage)\n"
     ]
    }
   ],
   "source": [
    "# Get list of non-null HTIDs\n",
    "vols = list(meta[~meta.HATHI_ID.isnull()].HATHI_ID.unique())\n",
    "print('Volumes with HTID:', len(vols), \"(of %s total;\"%meta.TITLE.count(), \n",
    "      '{:1.1f}'.format(100*meta.HATHI_ID.count()/meta.TITLE.count()), \"percent coverage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load geo data\n",
    "\n",
    "If `use_db` flag set to `True`, pull data from Postgres, else load from disk.\n",
    "\n",
    "We have two different data sets to load. One uses text strings -- the places identified via NER in the source texts -- but not geolocation data. This is more fine-grained, but doesn't know anything about geographic relations. \"South Bend,\" \"Indianapolis,\" and \"Indiana\" are three records as like (or unlike) as \"Chicago,\" \"Afghanistan,\" and \"Pacific Ocean.\"\n",
    "\n",
    "The second data set relies on geolocation data and collapses locations into high-level areas (states, countries, and continents). It also maintains a modest number of raw strings for colloquial and non-specific locations such as \"Pacific\" and \"New England\" that aren't handled well by geolocation. This set sacrifices some accuracy (geolocation is error prone) and specificity for better relationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Postgres connection if using DB\n",
    "#  Also need to set up SSH connection to server:\n",
    "#   ssh mwilkens@geolit.crc.nd.edu -L 5433:localhost:5433\n",
    "\n",
    "if use_db:\n",
    "    import sqlalchemy\n",
    "    from sqlalchemy import types\n",
    "    from sqlalchemy.sql import text\n",
    "\n",
    "    # Get credentials from file\n",
    "    try:\n",
    "        pg_username, pg_password = open(postgres_credentials).read().strip().split(' ')\n",
    "    except:\n",
    "        sys.exit('Cannot get Postgres credentials. Exiting.')\n",
    "\n",
    "    # Set up engine and connect\n",
    "    try:\n",
    "        pg_engine_string = 'postgresql+psycopg2://'+pg_username+':'+pg_password+'@'+postgres_server+':'+postgres_port+'/'+postgres_db_name\n",
    "        pg_engine = sqlalchemy.create_engine(pg_engine_string, echo=False)\n",
    "        #pg_metadata = sqlalchemy.MetaData()\n",
    "        pg_conn = pg_engine.connect()\n",
    "        print(\"Postgres connection established.\")\n",
    "    except:\n",
    "        sys.exit('Cannot make initial connection to Postgres database. Aborting.')\n",
    "    \n",
    "    # Raw strings\n",
    "    query_strings = text(\"SELECT htid, text_string \"\n",
    "                          \"FROM txtdata_incopyright \"\n",
    "                          \"WHERE htid IN :s\")\n",
    "    result_strings = pg_conn.execute(query_strings, s=tuple(vols))\n",
    "    output_strings = result_strings.fetchall()\n",
    "    print(\"Number of raw string records found:\", len(output_strings))\n",
    "    data_strings = pd.DataFrame(output_strings)\n",
    "    data_strings.columns = ['htid', 'text']\n",
    "    # Save output to file for later use\n",
    "    data_strings.to_csv(os.path.join(results_dir, 'location_strings.tsv'), \n",
    "                        sep='\\t', index=False)\n",
    "    \n",
    "    # Geo data\n",
    "    query_geo = text(\"SELECT htid, occurs, continent, country_short, admin_1_short, locality, text_string \"\n",
    "              \"FROM results_full \"\n",
    "              \"WHERE htid IN :s\")\n",
    "    result_geo = pg_conn.execute(query_geo, s=tuple(vols))\n",
    "    output_geo = result_geo.fetchall()\n",
    "    print(\"Number of geo data records found:\", len(output_geo))\n",
    "    data_geo = pd.DataFrame(output_geo)\n",
    "    data_geo.columns = ['htid', 'occurs', 'continent', 'country', 'admin_1', 'locality', 'text_string']\n",
    "    # Save output to file for later use\n",
    "    data_geo.to_csv(os.path.join(results_dir, 'location_geodata.tsv'), \n",
    "                    sep='\\t', index=False)\n",
    "    pg_conn.close()\n",
    "else: # Read data from disk if not using DB\n",
    "    data_strings = pd.read_csv(os.path.join(results_dir, 'location_strings.tsv'), sep='\\t')\n",
    "    data_geo = pd.read_csv(os.path.join(results_dir, 'location_geodata.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using raw NER text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>htid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mdp.39015038551183</td>\n",
       "      <td>Square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mdp.39015038551183</td>\n",
       "      <td>Goggleye Lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdp.39015038551183</td>\n",
       "      <td>Rome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mdp.39015038551183</td>\n",
       "      <td>Decatur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdp.39015038551183</td>\n",
       "      <td>Canadian River</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 htid            text\n",
       "0  mdp.39015038551183          Square\n",
       "1  mdp.39015038551183   Goggleye Lake\n",
       "2  mdp.39015038551183            Rome\n",
       "3  mdp.39015038551183         Decatur\n",
       "4  mdp.39015038551183  Canadian River"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the data\n",
    "data_strings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this data in the form of two lists:\n",
    "\n",
    "* HTIDs, one per volume\n",
    "* Strings used as locations in each volume, each as a tokenized list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data_strings.groupby('htid')\n",
    "translator = str.maketrans('\\n', ' ', string.punctuation) # to remove newlines in string data\n",
    "htids_strings = []\n",
    "geostrings = []\n",
    "for name, group in g:\n",
    "    htids_strings.append(name)\n",
    "    clean_strings = []\n",
    "    for i in list(group.text):\n",
    "        clean_strings.append(i.translate(translator).lower()) # remove \\n and lowercase\n",
    "    geostrings.append(clean_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of our lists: 1333 1333\n",
      "\n",
      "The first record in each list:\n",
      "inu.30000001734023\n",
      "['los angeles', 'oconnell bridge', 'south america', 'invara', 'bayswater', 'connemara', 'goolin', 'blanchardstown', 'rumania', 'oberammergau', 'n manchester', 'palermo', 'mass', 'brazil', 'astrakhan', 'clontarf', 'vienna', 'killarney', 'movita', 'hong kong', 'dolier street', 'canada', 'new york', 'dawson street', 'gresham', 'sicily', 'south america', 'dun laoghaire', 'baba', 'oranmore', 'mass', 'harcourt street', 'soho', 'bavaria', 'nenagh', 'caithleen', 'africa', 'limerick', 'molesworth street', 'tipperary', 'europe', 'china', 'india', 'new england', 'richmond hospital', 'golden marie', 'maryborough', 'america', 'grafton street', 'west of ireland', 'honolulu', 'grangegorman', 'california', 'galway', 'london', 'killaloe', 'mount melleray', 'dawson street', 'oconnell street', 'dublin', 'piccadilly', 'ireland', 'united states of america', 'cobh', 'spain', 'england', 'toronto', 'liverpool', 'pacific', 'collinstown', 'bengal', 'england', 'italy', 'hollywood', 'glencree', 'amiens street', 'pernod', 'ballinasloe', 'austin', 'shannon', 'north frederick street']\n"
     ]
    }
   ],
   "source": [
    "# Glance at the data\n",
    "print(\"Dimensions of our lists:\", len(htids_strings), len(geostrings))\n",
    "print(\"\\nThe first record in each list:\")\n",
    "print(htids_strings[0])\n",
    "print(geostrings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn` to turn our string data into a doc-term matrix. IDF weighting is optional and turned on here.\n",
    "\n",
    "Note that we bypass preprocessing and tokenization, since that's already handled for us by the orignal NER and by our work above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimenions of the doc-term matrix: (1333, 14029)\n"
     ]
    }
   ],
   "source": [
    "# define vectorizer parameters\n",
    "tfidf_vectorizer_preprocessed = TfidfVectorizer(max_df=1.0, min_df=2, use_idf=True, norm = 'l2',\n",
    "                                   lowercase = True, analyzer = 'word', \n",
    "                                   preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "tfidf_matrix_from_strings = tfidf_vectorizer_preprocessed.fit_transform(geostrings) #fit the vectorizer\n",
    "\n",
    "# Get the vocabulary for later use\n",
    "geo_terms_from_strings = tfidf_vectorizer_preprocessed.get_feature_names()\n",
    "print(\"Dimenions of the doc-term matrix:\", tfidf_matrix_from_strings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using geolocation data\n",
    "\n",
    "Use continent-country-state data rather than raw strings. Should give better sense of general orientation, but contains more errors and has no data for some locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>htid</th>\n",
       "      <th>occurs</th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>admin_1</th>\n",
       "      <th>locality</th>\n",
       "      <th>text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IE</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Amiens Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RU</td>\n",
       "      <td>Astrakhan Oblast</td>\n",
       "      <td>Astrakhan</td>\n",
       "      <td>Astrakhan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Austin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 htid  occurs continent country           admin_1   locality  \\\n",
       "0  inu.30000001734023       1    Africa     NaN               NaN        NaN   \n",
       "1  inu.30000001734023      10       NaN      US               NaN        NaN   \n",
       "2  inu.30000001734023       1       NaN      IE            Dublin     Dublin   \n",
       "3  inu.30000001734023       1       NaN      RU  Astrakhan Oblast  Astrakhan   \n",
       "4  inu.30000001734023       1       NaN      US                TX     Austin   \n",
       "\n",
       "     text_string  \n",
       "0         Africa  \n",
       "1        America  \n",
       "2  Amiens Street  \n",
       "3      Astrakhan  \n",
       "4         Austin  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glance at the data\n",
    "data_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Geo words'\n",
    "\n",
    "We want to aggregate locations by administrative area at a pretty high level. We collapse all locations into their country, except:\n",
    "\n",
    "* For US and GB, collapse to \"`admin_1`\" level (US state, GB country, e.g., 'England' or 'Scotland')\n",
    "* For continent references, keep as is. Note that we do not have continent data for locations that are not simple continent references, e.g., 'Africa' is labeled as a continent, but 'Nigeria' has no continent data. Not a problem, just something to know.\n",
    "* For locations that are hard to collapse appropriately ('Pacific Ocean', 'Danube'), keep as is. Would be nice to be able to get continent or country data for some of these, but it doesn't exist in our sources. This is a smallish issue, since the total number of records in this class is small.\n",
    "\n",
    "To do all this, we build a 'geoword' for each location. This 'geoword' is just the continent, country, and/or state label as appropriate, except for the small number of original text strings for the difficult cases. We then assemble the geowords into pseudodocuments to process in the same way we did with the raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>htid</th>\n",
       "      <th>occurs</th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>admin_1</th>\n",
       "      <th>locality</th>\n",
       "      <th>text_string</th>\n",
       "      <th>geoword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>America</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IE</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Amiens Street</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RU</td>\n",
       "      <td>Astrakhan Oblast</td>\n",
       "      <td>Astrakhan</td>\n",
       "      <td>Astrakhan</td>\n",
       "      <td>RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>Austin</td>\n",
       "      <td>US-TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>MA</td>\n",
       "      <td>Worcester</td>\n",
       "      <td>Baba</td>\n",
       "      <td>US-MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IE</td>\n",
       "      <td>Galway</td>\n",
       "      <td>Ballinasloe</td>\n",
       "      <td>Ballinasloe</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DE</td>\n",
       "      <td>BY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bavaria</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Bayswater</td>\n",
       "      <td>US-CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inu.30000001734023</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>LA</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>Bengal</td>\n",
       "      <td>US-LA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 htid  occurs continent country           admin_1  \\\n",
       "0  inu.30000001734023       1    Africa     NaN               NaN   \n",
       "1  inu.30000001734023      10       NaN      US               NaN   \n",
       "2  inu.30000001734023       1       NaN      IE            Dublin   \n",
       "3  inu.30000001734023       1       NaN      RU  Astrakhan Oblast   \n",
       "4  inu.30000001734023       1       NaN      US                TX   \n",
       "5  inu.30000001734023       1       NaN      US                MA   \n",
       "6  inu.30000001734023       1       NaN      IE            Galway   \n",
       "7  inu.30000001734023       1       NaN      DE                BY   \n",
       "8  inu.30000001734023       1       NaN      US                CO   \n",
       "9  inu.30000001734023       1       NaN      US                LA   \n",
       "\n",
       "      locality    text_string geoword  \n",
       "0          NaN         Africa  Africa  \n",
       "1          NaN        America      US  \n",
       "2       Dublin  Amiens Street      IE  \n",
       "3    Astrakhan      Astrakhan      RU  \n",
       "4       Austin         Austin   US-TX  \n",
       "5    Worcester           Baba   US-MA  \n",
       "6  Ballinasloe    Ballinasloe      IE  \n",
       "7          NaN        Bavaria      DE  \n",
       "8       Denver      Bayswater   US-CO  \n",
       "9  Baton Rouge         Bengal   US-LA  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specials = ('US', 'GB') # Countries to include admin_1\n",
    "\n",
    "# Set geoword to continent, country, or country-admin_1 as appropriate\n",
    "data_geo['geoword'] = np.where(~data_geo.continent.isnull(),\n",
    "                               data_geo.continent,\n",
    "                               np.where(data_geo.country.isin(specials) & ~data_geo.admin_1.isnull(),\n",
    "                                   data_geo.country+'-'+data_geo.admin_1,\n",
    "                                   data_geo.country)\n",
    "                              )\n",
    "# Set missing geowords to text string (mostly, e.g., 'New England', 'Mediterranean', etc.)\n",
    "data_geo['geoword'] = np.where(~data_geo.geoword.isnull(),\n",
    "                               data_geo.geoword,\n",
    "                               data_geo.text_string.apply(lambda x: x.translate(translator))\n",
    "                              )\n",
    "data_geo.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build lists of HTIDs and geowords. Note that we need to repeat each geoword as many times as it appears in each text (using data from the 'occurs' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gg = data_geo.groupby('htid')\n",
    "htids_geo = []\n",
    "geowords = []\n",
    "for name, group in gg:\n",
    "    htids_geo.append(name)\n",
    "    geowords_list = []\n",
    "    for item in group.itertuples():\n",
    "        for i in range(item.occurs):\n",
    "            geowords_list.append(item.geoword)\n",
    "    geowords.append(geowords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inu.30000001734023\n",
      "['Africa', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'IE', 'RU', 'US-TX', 'US-MA', 'IE', 'DE', 'US-CO', 'US-LA', 'IE', 'IE', 'BR', 'BR', 'US-CA', 'CA', 'CN', 'CN', 'IE', 'IE', 'IE', 'IE', 'US-KY', 'IE', 'US-NC', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'Europe', 'IE', 'IE', 'US-GA', 'US-VA', 'US-VA', 'US-VA', 'US-VA', 'IE', 'US-OR', 'IE', 'US-CA', 'US-CA', 'US-CA', 'HK', 'US-HI', 'IN', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IE', 'IT', 'IE', 'IE', 'IE', 'IE', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'US-PA', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'GB-England', 'US-CA', 'AU', 'US-MA', 'US-MA', 'US-MA', 'IE', 'IE', 'IE', 'IE', 'New England', 'US-NY', 'US-NY', 'US-NY', 'US-NY', 'US-NY', 'US-NY', 'US-IN', 'DE', 'IE', 'IE', 'IE', 'US-IL', 'IT', 'US-AR', 'US-AR', 'US-TN', 'US-OH', 'RO', 'US-TX', 'US-TX', 'US-TX', 'IT', 'IT', 'IT', 'US-NY', 'US-NY', 'US-NY', 'South America', 'South America', 'South America', 'South America', 'ES', 'IE', 'CA', 'US', 'AT', 'AT', 'IE']\n"
     ]
    }
   ],
   "source": [
    "# Glance at the processed data\n",
    "print(htids_geo[0])\n",
    "print(geowords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total geowords:  572789\n",
      "Unique geowords: 831\n",
      "\n",
      "Most frequently occurring geowords:\n",
      "US-NY \t 51800\n",
      "US-CA \t 41036\n",
      "FR \t 25863\n",
      "GB-England \t 25734\n",
      "US \t 19730\n",
      "IT \t 16548\n",
      "US-TX \t 13426\n",
      "RU \t 12844\n",
      "US-FL \t 12348\n",
      "US-MA \t 12175\n"
     ]
    }
   ],
   "source": [
    "# Print most frequent geowords, for reference only\n",
    "total_geowords = 0\n",
    "geoword_counts = {}\n",
    "for i in geowords:\n",
    "    for token in i:\n",
    "        total_geowords += 1\n",
    "        try:\n",
    "            geoword_counts[token] += 1\n",
    "        except:\n",
    "            geoword_counts[token] = 1\n",
    "top = sorted(geoword_counts, key=geoword_counts.get, reverse = True)\n",
    "\n",
    "print(\"Total geowords: \", total_geowords)\n",
    "print(\"Unique geowords:\", len(geoword_counts.keys()))\n",
    "print(\"\\nMost frequently occurring geowords:\")\n",
    "for i in range(10):\n",
    "    print(top[i], '\\t', geoword_counts[top[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output matrix: (1333, 572)\n"
     ]
    }
   ],
   "source": [
    "# Fit the data\n",
    "tfidf_matrix_from_geo_data = tfidf_vectorizer_preprocessed.fit_transform(geowords)\n",
    "\n",
    "# Get the vocabulary\n",
    "geo_terms_from_geo_data = tfidf_vectorizer_preprocessed.get_feature_names()\n",
    "print(\"Shape of the output matrix:\", tfidf_matrix_from_geo_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA to generate geo features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on text strings\n",
      "(0) Sum of variance explained: 0\n",
      "(1) Sum of variance explained: 0.00859547571077\n",
      "(2) Sum of variance explained: 0.0145316134713\n",
      "(3) Sum of variance explained: 0.0193796609091\n",
      "(4) Sum of variance explained: 0.0236694662307\n",
      "(5) Sum of variance explained: 0.0276644800575\n",
      "(6) Sum of variance explained: 0.0310490030162\n",
      "(7) Sum of variance explained: 0.0342711172846\n",
      "(8) Sum of variance explained: 0.037355087152\n",
      "(9) Sum of variance explained: 0.0403189099379\n",
      "\n",
      "Performance on geo data\n",
      "(0) Sum of variance explained: 0\n",
      "(1) Sum of variance explained: 0.0836067964635\n",
      "(2) Sum of variance explained: 0.148963279184\n",
      "(3) Sum of variance explained: 0.196212313231\n",
      "(4) Sum of variance explained: 0.234077645998\n",
      "(5) Sum of variance explained: 0.263305967001\n",
      "(6) Sum of variance explained: 0.290626733454\n",
      "(7) Sum of variance explained: 0.312947131911\n",
      "(8) Sum of variance explained: 0.334218613521\n",
      "(9) Sum of variance explained: 0.35314557012\n"
     ]
    }
   ],
   "source": [
    "components = 100 # Number of principal components to fit\n",
    "pca_strings = PCA(n_components=components)\n",
    "pca_geodata = PCA(n_components=components)\n",
    "strings_pca = pca_strings.fit_transform(tfidf_matrix_from_strings.toarray())\n",
    "geodata_pca = pca_geodata.fit_transform(tfidf_matrix_from_geo_data.toarray())\n",
    "\n",
    "print(\"Performance on text strings\")\n",
    "for i in range(10):\n",
    "    pctvar = sum(pca_strings.explained_variance_ratio_[0:i])\n",
    "    print(\"(%s) Sum of variance explained: %s\" % (i, pctvar))\n",
    "    \n",
    "print(\"\\nPerformance on geo data\")\n",
    "for i in range(10):\n",
    "    pctvar = sum(pca_geodata.explained_variance_ratio_[0:i])\n",
    "    print(\"(%s) Sum of variance explained: %s\" % (i, pctvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obviously capture a lot more of the underlying variance in the geowords case. Not surprising, given that we're reducing from 572 features rather than 14,029 in the strings case.\n",
    "\n",
    "Could consider using other clustering/distance/dimensionality reduction methods to generate geo features if desired. Not pursuing at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top loadings for first ten PCs of string data\n",
      "PC 0: ['new york city' 'la' 'los angeles' 'san francisco' 'new jersey' 'florida'\n",
      " 'chicago' 'california' 'broadway' 'manhattan' 'connecticut' 'arizona'\n",
      " 'texas' 'ohio' 'new orleans']\n",
      "PC 1: ['fifth avenue' 'central park' 'long island' 'manhattan' 'park avenue'\n",
      " 'broadway' 'brooklyn' 'east river' 'lower east side' 'bronx' 'queens'\n",
      " 'west side' 'central park west' 'times square' 'east side']\n",
      "PC 2: ['san francisco' 'la' 'los angeles' 'san diego' 'santa monica' 'hong kong'\n",
      " 'beverly hills' 'santa barbara' 'pacific' 'sacramento' 'moscow'\n",
      " 'soviet union' 'hawaii' 'southern california' 'asia']\n",
      "PC 3: ['new orleans' 'mississippi' 'alabama' 'tennessee' 'south' 'north carolina'\n",
      " 'memphis' 'georgia' 'south carolina' 'kentucky' 'arkansas' 'atlanta'\n",
      " 'louisiana' 'soviet union' 'dc']\n",
      "PC 4: ['ireland' 'scotland' 'cambridge' 'oxford' 'britain' 'wales' 'dublin'\n",
      " 'victoria' 'london' 'oxford street' 'brighton' 'new england' 'yorkshire'\n",
      " 'great britain' 'manchester']\n",
      "PC 5: ['new orleans' 'major de spain' 'memphis' 'la' 'st louis' 'mississippi'\n",
      " 'jefferson' 'yoknapatawpha county' 'vicksburg' 'santa monica' 'chickasaw'\n",
      " 'spain' 'tennessee' 'arkansas' 'fifth avenue']\n",
      "PC 6: ['hong kong' 'soho' 'memphis' 'new orleans' 'cuba' 'times square' 'houston'\n",
      " 'birmingham' 'central park west' 'central park' 'afghanistan' 'vietnam'\n",
      " 'southeast asia' 'australia' 'britain']\n",
      "PC 7: ['egypt' 'jerusalem' 'syria' 'middle east' 'babylon' 'earth' 'israel'\n",
      " 'damascus' 'athens' 'mediterranean' 'lebanon' 'palestine' 'jericho'\n",
      " 'alexandria' 'jordan']\n",
      "PC 8: ['golden gate bridge' 'golden gate park' 'san francisco' 'north beach'\n",
      " 'new england' 'sausalito' 'pacific heights' 'marin county' 'bay bridge'\n",
      " 'golden gate' 'new orleans' 'oakland' 'san francisco bay' 'berkeley'\n",
      " 'mass']\n",
      "PC 9: ['jerusalem' 'israel' 'persia' 'west' 'kansas city' 'east' 'pa'\n",
      " 'philadelphia' 'jericho' 'arabia' 'st louis' 'galilee' 'syria'\n",
      " 'st petersburg' 'jordan']\n",
      "\n",
      "Top loadings for first ten PCs of geo data\n",
      "PC 0: ['US-NY' 'US-CA' 'US-MA' 'US-IL' 'US' 'US-PA' 'US-NJ' 'US-IN' 'US-CT'\n",
      " 'US-FL' 'US-WA' 'US-OH' 'US-ME' 'US-MO' 'US-VT']\n",
      "PC 1: ['US-CA' 'MX' 'US-TX' 'US-NV' 'US-AZ' 'US-NM' 'US-WA' 'US-CO' 'US-HI'\n",
      " 'US-IL' 'US-WY' 'US-OH' 'US-TN' 'US-OR' 'US-LA']\n",
      "PC 2: ['GB-England' 'US-CA' 'FR' 'GB' 'IT' 'US-NY' 'IE' 'GB-Scotland' 'Europe'\n",
      " 'GB-Wales' 'CH' 'BE' 'NZ' 'US-NV' 'ES']\n",
      "PC 3: ['FR' 'IT' 'RU' 'DE' 'US-CA' 'CH' 'ES' 'Europe' 'AT' 'IL' 'US-NY' 'PL' 'UA'\n",
      " 'EG' 'DZ']\n",
      "PC 4: ['RU' 'DE' 'US-NY' 'US-CA' 'GB-England' 'IL' 'UA' 'IT' 'JP' 'PL' 'IN' 'CN'\n",
      " 'AT' 'Siberia' 'GB']\n",
      "PC 5: ['IT' 'US-MA' 'IL' 'GR' 'VA' 'GB' 'EG' 'GB-England' 'IN' 'CH' 'ZA' 'SY'\n",
      " 'US-MN' 'US-DE' 'TR']\n",
      "PC 6: ['US-MA' 'RU' 'IT' 'US-VA' 'US-NC' 'US-PA' 'US-FL' 'US-MD' 'US-WA' 'US-IL'\n",
      " 'US-ME' 'US-NH' 'US-CT' 'US' 'DE']\n",
      "PC 7: ['US-MA' 'JP' 'CN' 'IN' 'IL' 'US-ME' 'US-NH' 'FR' 'US-CT' 'IE' 'US-HI'\n",
      " 'US-VT' 'New England' 'GR' 'HK']\n",
      "PC 8: ['DE' 'US-NC' 'IL' 'US-FL' 'US-VA' 'PL' 'US-PA' 'US-GA' 'CH' 'US' 'AT'\n",
      " 'US-NJ' 'US-DC' 'US-TN' 'US-WA']\n",
      "PC 9: ['US-NC' 'US-FL' 'US-GA' 'US-VA' 'CN' 'JP' 'US-TN' 'FR' 'IN' 'US-CA' 'RU'\n",
      " 'US-NY' 'IE' 'US-SC' 'GR']\n"
     ]
    }
   ],
   "source": [
    "# Get PC loadings\n",
    "loadings_strings = pca_strings.components_\n",
    "loadings_strings_df = pd.DataFrame(loadings_strings.T, index=geo_terms_from_strings)\n",
    "loadings_strings_df.to_csv(os.path.join(results_dir, 'loadings_strings.tsv'), sep='\\t')\n",
    "\n",
    "loadings_geodata = pca_geodata.components_\n",
    "loadings_geodata_df = pd.DataFrame(loadings_geodata.T, index=geo_terms_from_geo_data)\n",
    "loadings_geodata_df.to_csv(os.path.join(results_dir, 'loadings_geodata.tsv'), sep='\\t')\n",
    "\n",
    "# show top n load items for first 10 PCs\n",
    "print(\"Top loadings for first ten PCs of string data\")\n",
    "for i in range(10):\n",
    "    print(\"PC %s:\"%i, loadings_strings_df[i].nlargest(15).index.values)\n",
    "    \n",
    "print(\"\\nTop loadings for first ten PCs of geo data\")\n",
    "for i in range(10):\n",
    "    print(\"PC %s:\"%i, loadings_geodata_df[i].nlargest(15).index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build and save results frames\n",
    "string_features = pd.DataFrame(strings_pca, index=htids_strings)\n",
    "string_features.to_csv(os.path.join(results_dir, 'features_strings.csv'))\n",
    "geodata_features = pd.DataFrame(geodata_pca, index=htids_geo)\n",
    "geodata_features.to_csv(os.path.join(results_dir, 'features_geodata.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "Presumably want these features for use with other data. Below is a quick example of joining the output frame here to the existing metadata frame. Same operation would be used to join other feature data stored in other frames.\n",
    "\n",
    "Couple of notes:\n",
    "\n",
    "* Example below assumes you *don't* want all the PCs. It's limited to the first three. Adjust as desired.\n",
    "* The example join ignores rows in the metadata frame that do not have an HTID on which to join. This is probably desired behavior, but be aware of it. To retain (with attendant NaNs in the result, change `how='right'` to `how='left'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare data set sizes:\n",
      "Volumes with geo data: 1333\n",
      "Volumes in original metadata: 3525\n",
      "Volumes in integrated output: 1333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>HATHI_ID</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>RACE</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>GENRE</th>\n",
       "      <th>YEAR_CORRECT</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sunrise to sunset / | $c: [by] Samuel Hopkins ...</td>\n",
       "      <td>Adams, Samuel Hopkins,</td>\n",
       "      <td>uc1.$b86201</td>\n",
       "      <td>New York|Random House|1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>ROM</td>\n",
       "      <td>1950</td>\n",
       "      <td>0.504608</td>\n",
       "      <td>-0.137849</td>\n",
       "      <td>0.029980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wait for the dawn | $c: [by] Martha Albrand [p...</td>\n",
       "      <td>Albrand, Martha.</td>\n",
       "      <td>uc1.$b120100</td>\n",
       "      <td>New York|Random House|1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>F</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>ROM</td>\n",
       "      <td>1950</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.190282</td>\n",
       "      <td>-0.034396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The delicate prey, | and other stories.</td>\n",
       "      <td>Bowles, Paul,</td>\n",
       "      <td>mdp.39015054095990</td>\n",
       "      <td>New York|Random House|1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>American</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1950</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.051127</td>\n",
       "      <td>-0.050132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The watchful gods, | and other stories.</td>\n",
       "      <td>Clark, Walter Van Tilburg,</td>\n",
       "      <td>mdp.39015013110138</td>\n",
       "      <td>New York|Random House|1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>M</td>\n",
       "      <td>white</td>\n",
       "      <td>American</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1950</td>\n",
       "      <td>-0.062978</td>\n",
       "      <td>0.137239</td>\n",
       "      <td>-0.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Collected stories / | $c: William Faulkner.</td>\n",
       "      <td>Faulkner, William,</td>\n",
       "      <td>mdp.39015000689193</td>\n",
       "      <td>New York|Random House|1950.</td>\n",
       "      <td>1950</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>american</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1950</td>\n",
       "      <td>-0.086734</td>\n",
       "      <td>-0.111916</td>\n",
       "      <td>0.024281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE  \\\n",
       "19  Sunrise to sunset / | $c: [by] Samuel Hopkins ...   \n",
       "20  Wait for the dawn | $c: [by] Martha Albrand [p...   \n",
       "21            The delicate prey, | and other stories.   \n",
       "22            The watchful gods, | and other stories.   \n",
       "23        Collected stories / | $c: William Faulkner.   \n",
       "\n",
       "                        AUTHOR            HATHI_ID  \\\n",
       "19      Adams, Samuel Hopkins,         uc1.$b86201   \n",
       "20            Albrand, Martha.        uc1.$b120100   \n",
       "21               Bowles, Paul,  mdp.39015054095990   \n",
       "22  Clark, Walter Van Tilburg,  mdp.39015013110138   \n",
       "23          Faulkner, William,  mdp.39015000689193   \n",
       "\n",
       "                      PUBLISHER  YEAR GENDER       RACE NATIONALITY GENRE  \\\n",
       "19   New York|Random House|1950  1950      M  Caucasian    American   ROM   \n",
       "20   New York|Random House|1950  1950      F  Caucasian    American   ROM   \n",
       "21   New York|Random House|1950  1950      M  Caucasian    American   LIT   \n",
       "22   New York|Random House|1950  1950      M      white    American   LIT   \n",
       "23  New York|Random House|1950.  1950   male      white    american   LIT   \n",
       "\n",
       "   YEAR_CORRECT         0         1         2  \n",
       "19         1950  0.504608 -0.137849  0.029980  \n",
       "20         1950  0.055164  0.190282 -0.034396  \n",
       "21         1950  0.009584  0.051127 -0.050132  \n",
       "22         1950 -0.062978  0.137239 -0.071700  \n",
       "23         1950 -0.086734 -0.111916  0.024281  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of hooking up these results with other meta/feature data\n",
    "pcs_to_use = 3 # Keep this many PCs in our integrated data\n",
    "data_to_join = geodata_features.loc[:,0:(pcs_to_use-1)]\n",
    "integrated_results = meta.join(data_to_join, on='HATHI_ID', how='right')\n",
    "\n",
    "print(\"Compare data set sizes:\")\n",
    "print(\"Volumes with geo data:\", len(geodata_features.index))\n",
    "print(\"Volumes in original metadata:\", len(meta.index))\n",
    "print(\"Volumes in integrated output:\", len(integrated_results.index))\n",
    "integrated_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
